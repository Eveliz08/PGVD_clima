import streamlit as st
import pandas as pd

def show():
    """P√°gina para limpiar y procesar el dataset."""
    
    st.title("üßπ Limpiar Dataset")
    
    exists, _ = check_file_exists_in_hdfs()
    if not exists:
        st.warning("‚ö†Ô∏è Por favor, carga un dataset primero en el m√≥dulo 'Cargar Dataset'")
        return
    
    # Inicializar Spark si no existe
    if 'spark' not in st.session_state:
        st.session_state.spark = SparkSession.builder.appName("ClimaDashboard").getOrCreate()
    
    spark = st.session_state.spark
    
    # Cargar datos desde HDFS si no est√°n en sesi√≥n
    if 'df_spark' not in st.session_state:
        with st.spinner("Cargando datos desde HDFS..."):
            try:
                df_spark = spark.read.csv("hdfs://namenode:9000/clima/GlobalLandTemperaturesByCity.csv", header=True, inferSchema=True)
                st.session_state.df_spark = df_spark
                st.session_state.df = df_spark.limit(1000).toPandas()  # Sample para display
                st.success("‚úÖ Datos cargados desde HDFS")
            except Exception as e:
                st.error(f"‚ùå Error cargando datos: {e}")
                return
    
    df_spark = st.session_state.df_spark
    
    # Aplicar normalizaci√≥n autom√°ticamente
    if 'normalized' not in st.session_state:
        with st.spinner("Aplicando normalizaci√≥n..."):
            try:
                df_spark = normalize(df_spark)
                st.session_state.df_spark = df_spark
                st.session_state.df = df_spark.limit(1000).toPandas()
                st.session_state.normalized = True
            except Exception as e:
                st.error(f"‚ùå Error en normalizaci√≥n: {e}")
                return
    
    # Ejecutar preEDA (usa la clase PreEDAAnalyzer)
    analyzer = PreEDAAnalyzer(spark)
    if 'preEDA_stats' not in st.session_state:
        with st.spinner("Ejecutando preEDA..."):
            try:
                stats_df, plots = analyzer.analyze("hdfs://namenode:9000/clima/GlobalLandTemperaturesByCity.csv")
                st.session_state.preEDA_stats = stats_df
                st.session_state.preEDA_plots = plots
            except Exception as e:
                st.error(f"‚ùå Error en preEDA: {e}")
                return
    
    stats_df = st.session_state.preEDA_stats
    
    st.markdown("---")
    
    # Mostrar estad√≠sticas preEDA
    st.subheader("üìä Estado del Dataset (preEDA)")
    st.dataframe(stats_df, use_container_width=True)
    
    # Bot√≥n para limpiar (clean e imputer)
    if st.button("üßπ Limpiar Dataset"):
        with st.spinner("Aplicando limpieza y imputaci√≥n..."):
            try:
                df_spark = clean(df_spark)
                df_spark = imputer(df_spark)
                st.session_state.df_spark = df_spark
                st.session_state.df = df_spark.limit(1000).toPandas()
                st.success("‚úÖ Limpieza y imputaci√≥n aplicadas")
                # Recalcular preEDA usando el analizador sobre la ruta en HDFS
                stats_df, plots = analyzer.analyze("hdfs://namenode:9000/clima/GlobalLandTemperaturesByCity.csv")
                st.session_state.preEDA_stats = stats_df
                st.session_state.preEDA_plots = plots
            except Exception as e:
                st.error(f"‚ùå Error en limpieza: {e}")
    
    st.markdown("---")
    
    # tab1, tab2, tab3, tab4 = st.tabs(use_container_width
    #     ["Valores Faltantes", "Outliers", "Normalizaci√≥n", "Transformaciones"]
    # )
    
    # with tab1:
    #     st.subheader("Manejo de Valores Faltantes")
        
    #     col1, col2 = st.columns(2)
        
    #     with col1:
    #         st.markdown("**Resumen de valores faltantes**")
    #         missing = st.session_state.df.isnull().sum()
    #         st.dataframe(missing[missing > 0], use_container_width=True)
        
    #     with col2:
    #         method = st.selectbox(
    #             "Selecciona m√©todo de imputaci√≥n",
    #             ["Media", "Mediana", "Moda", "KNN"]
    #         )
            
    #         if st.button("Aplicar imputaci√≥n"):
    #             st.success(f"‚úÖ Imputaci√≥n aplicada usando {method}")
    
    # with tab2:
    #     st.subheader("Detecci√≥n y Manejo de Outliers")
        
    #     column = st.selectbox("Selecciona columna", st.session_state.df.columns)
    #     method = st.selectbox(
    #         "M√©todo de detecci√≥n",
    #         ["IQR", "Z-Score", "Isolation Forest"]
    #     )
        
    #     if st.button("Detectar outliers"):
    #         st.info(f"Detectando outliers en {column} usando {method}...")
    
    # with tab3:
    #     st.subheader("Normalizaci√≥n de Datos")
        
    #     norm_method = st.selectbox(
    #         "Selecciona m√©todo de normalizaci√≥n",
    #         ["Min-Max (0-1)", "Z-Score", "Robust Scaler"]
    #     )
        
    #     columns = st.multiselect(
    #         "Columnas a normalizar",
    #         st.session_state.df.select_dtypes(include=['float64', 'int64']).columns
    #     )
        
    #     if st.button("Aplicar normalizaci√≥n"):
    #         st.success(f"‚úÖ Normalizaci√≥n {norm_method} aplicada")
    
    # with tab4:
    #     st.subheader("Transformaciones Adicionales")
        
    #     col1, col2 = st.columns(2)
        
    #     with col1:
    #         if st.checkbox("Eliminar duplicados"):
    #             st.success("‚úÖ Duplicados eliminados")
            
    #         if st.checkbox("Convertir tipos de datos"):
    #             st.info("Configurar conversiones...")
        
    #     with col2:
    #         if st.checkbox("Crear variables derivadas"):
    #             st.info("Crear nuevas variables...")
            
    #         if st.checkbox("Filtrar datos"):
    #             st.info("Establecer criterios de filtrado...")
    
    # st.markdown("---")
    # st.markdown("**Resumen del dataset procesado**")
    # st.dataframe(st.session_state.df.head(), use_container_width=True)
